import os
import sys
import hashlib
import argparse
import fnmatch
import re
import time
import datetime
import textwrap # For better help formatting
import math
from typing import Optional


# --- Script Description ---
#
SCRIPT_VERSION = "v26.00"
SCRIPT_DATE    = "21-July-2025"
#
# A. ?????/??????
#
# This script compares two folders (source and target) to identify and report
# on items based on their existence, modification dates, and size.
#
# It provides a detailed report on the following exclusive categories:
#  1. Broken symbolic links found in the source folder.
#  2. Broken symbolic links found in the target folder.
#  3. Files existing only in the source folder.
#  4. Files existing only in the target folder.
#  5. Files existing in both source and target that are considered IDENTICAL.
#     - Identical means: (a) sizes are identical AND (b) timestamps are within
#       a user-defined time tolerance.
#     - An optional hash check (--hash-check) can be enabled for stricteri
#       identity verification, in which case files are identical ONLY if
#       their content hashes match (along with size).
#  6. Files existing in both source and target, which are DIFFERENT, andi
#     source is newer.
#  7. Files existing in both source and target, which are DIFFERENT, and
#     target is newer.
#  8. Files/Folders excluded based on --exclude patterns
#  9. Items that have different types (File/Folder) in Source an Target
# 10. Items for which an error has occurred during comparison
#
# The report includes the count and total size for each file category.
# Users can specify patterns to exclude certain files or folders from the
# comparison, either directly via command-line arguments or by providing a
# file containing the patterns.
#
# Usage examples:
# ---------------
# python3 folder_comparator.py --exclude "*.log", "toto/activate*", "temp_dir/" -- source_dir target_dir
# python3 folder_comparator.py --exclude-file my_exclusions.txt --time-tolerance 0.1 -hash-check md5 -- source_dir target_dir
#
# Note on timestamps (modification time):
# ---------------------------------------
# Sometimes, when manipulating files, the modification timestamp may be rounded
# to the nearest second, in which case the use of --time-tolerance 1 should
# indicate that such files should be treated as identical.
#
# Note on the logic for size calculation:
# ---------------------------------------
# For reporting and comparison purposes, item sizes are handled as follows:
#   - Regular Files: Their actual byte size is used.
#   - Directories: Have no meaningful "content size" in bytes.
#                  Their size is recorded as 'None' during scanning and
#                  displayed as 'None bytes' in the report.
#                  They are not included in total size sums.
#   - Symbolic Links: The size recorded is the actual size of the symlink
#                     entry itself (i.e., the byte length of the target
#                     path string it contains), not the size of its target.
#                     This size is a numerical value, displayed as 'XX bytes',
#                     and included in total size sums where applicable.
#   - Broken symlinks are reported separately and do not have a size
#     associated for comparison.
# This approach ensures clear distinction between actual file content size,
# the conceptual 'size' of directories, and the true footprint of symlinks.

#
# --- End of Script Description ---


# --- Constants for Comparison Outcomes and Report Categories ---
# These are the 10 definitive report categories
# The titles of the categories reported in the final report are defined in the print_report function
REPORT_BROKEN_SYMLINK_SOURCE = "Broken Symbolic Links found in Source."
REPORT_BROKEN_SYMLINK_TARGET = "Broken Symbolic Links found in Target."
REPORT_ONLY_IN_SOURCE = "Files existing ONLY in Source."
REPORT_ONLY_IN_TARGET = "Files existing ONLY in Target."
REPORT_IDENTICAL = "Files in Source AND Target that are IDENTICAL" # The detailed explanation will be built dynamically in print_report
REPORT_IDENTICAL_DIR = "Folders existing in Source AND Target that are IDENTICAL"
REPORT_DIFFERENT_CONTENT_SOURCE_NEWER = "Files in Source AND Target that are DIFFERENT, and SOURCE is newer."
REPORT_DIFFERENT_CONTENT_TARGET_NEWER = "Files in Source AND Target that are DIFFERENT, and TARGET is newer."
REPORT_EXCLUDED = "Excluded Files/Folders (based on --exclude patterns)."
REPORT_DIFFERENT_TYPE = "Items with Different Type (File vs. Folder) found in both Source and Target."
REPORT_COMPARE_ERROR = "Comparison Errors."

# Global dictionary to store all comparison results, categorized
comparison_results_dict = {
    REPORT_BROKEN_SYMLINK_SOURCE: [],
    REPORT_BROKEN_SYMLINK_TARGET: [],
    REPORT_ONLY_IN_SOURCE: [],
    REPORT_ONLY_IN_TARGET: [],
    REPORT_IDENTICAL: [],
    REPORT_DIFFERENT_CONTENT_SOURCE_NEWER: [],
    REPORT_DIFFERENT_CONTENT_TARGET_NEWER: [],
    REPORT_EXCLUDED: [],
    REPORT_DIFFERENT_TYPE: [],
    REPORT_COMPARE_ERROR: []
}

# --- Global variables for optimization and debugging ---
compiled_file_patterns_global = []
compiled_dir_patterns_global = []
compiled_root_specific_file_patterns_global = []
compiled_root_specific_dir_patterns_global = []
patterns_compiled_flag = False
DEBUG_EXCLUDE_ENABLED = False
DEBUG_TARGET_PATTERNS = [] # Populated from args

# --- Gloabl variables related to the exclusions patterns
all_exclude_patterns = []
num_patterns_from_exclude_file = 0

# --- Global variables for cleaned paths (set in main)
source_folder_cleaned = ""
target_folder_cleaned = ""

# --- Unified Data Structure for Comparison Items ---
class ComparisonItem:
    """
    Represents a single item (file or directory) from either source or target,
    along with all relevant comparison attributes. This unifies the data structure
    for all categories.
    """
    def __init__(self, relative_path: str):
        self.relative_path = relative_path
        # Attributes for Source
        self.exists_source: bool = False # Indicates if item exists in source
        self.is_dir_source: bool = False
        self.is_file_source: bool = False # Explicitly indicates if it's a file
        self.is_symlink_source: bool = False # Explicitly indicates if it's a symlink
        self.size_source: int or None = None
        self.mtime_ns_source: int or None = None
        self.hash_source: str or None = None
        self.symlink_target_source: str or None = None # Target path for source symlink
        self.is_broken_symlink_source: bool = False

        # Attributes for Target
        self.exists_target: bool = False # Indicates if item exists in target
        self.is_dir_target: bool = False
        self.is_file_target: bool = False # Explicitly indicates if it's a file
        self.is_symlink_target: bool = False # Explicitly indicates if it's a symlink
        self.size_target: int or None = None
        self.mtime_ns_target: int or None = None
        self.hash_target: str or None = None
        self.symlink_target_target: str or None = None # Target path for target symlink
        self.is_broken_symlink_target: bool = False

        # Comparison Flags and Status
        self.is_excluded_source: bool = False # True if excluded from source processing
        self.is_excluded_target: bool = False # True if excluded from target processing
        self.exclusion_matched_pattern: str or None = None # Pattern that caused exclusion
        self.type_mismatch_description: str or None = None # For REPORT_DIFFERENT_TYPE
        self.comparison_error_message: str or None = None # For REPORT_COMPARE_ERROR
        self.category_key: str or None = None # New: To store the final report category for this item

        # Attributes to capture initial scan issues, if any (pre-comparison)
        self.scan_error_source: str or None = None # Error message if source item couldn't be scanned
        self.scan_error_target: str or None = None # Error message if target item couldn't be scanned

# --- Core Utility Functions ---

def calculate_hash(file_path, hash_algorithm='md5', block_size=65536):
    """Calculates the hash of a file using the specified algorithm (md5 or sha256)."""
    if hash_algorithm == 'md5':
        hasher = hashlib.md5()
    elif hash_algorithm == 'sha256':
        hasher = hashlib.sha256()
    else:
        return None # Should not happen with argparse choices

    try:
        with open(file_path, 'rb') as f:
            for block in iter(lambda: f.read(block_size), b''):
                hasher.update(block)
        return hasher.hexdigest()
    except Exception as e:
        return None # Indicate failure to calculate hash

def _glob_to_regex_core(glob_pattern_normalized):
    """
    Converts a normalized global pattern into a regular expression.
    Handles wildcards ‘*’, ‘**’, ‘?’, and escapes other special regex characters.
    Adds NO anchors (^, $, or /) - these are added by the calling function.
    If ‘**’ is present in the global pattern, ‘*’ will also match slashes.
    """
    regex_parts = []

    has_recursive_wildcard = '**' in glob_pattern_normalized

    parts = glob_pattern_normalized.split('**')

    for i, part_containing_slashes in enumerate(parts):
        if i > 0: # If this is not the first part, it was preceded by '**'
            regex_parts.append('.*') # '**' matches everything, including slashes

        # Process the current part, which might contain literal slashes
        # The key here is how we handle leading slashes in parts, and how * and ? behave.

        # Split by literal slashes *without escaping them here yet*
        segments = part_containing_slashes.split('/')

        for j, segment in enumerate(segments):
            if j > 0: # Add a literal slash if not the first segment in this part
                regex_parts.append('/')

            # Escape regex special characters in the segment
            escaped_segment = re.escape(segment)

            # Replace glob wildcards with regex equivalents based on `has_recursive_wildcard`
            if has_recursive_wildcard:
                escaped_segment = escaped_segment.replace(re.escape('*'), '.*')
                escaped_segment = escaped_segment.replace(re.escape('?'), '.')
            else:
                escaped_segment = escaped_segment.replace(re.escape('*'), '[^/]*')
                escaped_segment = escaped_segment.replace(re.escape('?'), '[^/]')

            regex_parts.append(escaped_segment)

    # Special handling for patterns starting with '/': anchor to start of path
    # But `_glob_to_regex_core` should not add anchors, so this is just a comment for caller.

    return "".join(regex_parts)

def compile_exclusion_patterns(patterns_list):
    """
    Compile glob-style exclusion patterns into regular expressions.
    Separates patterns into file and directory lists, and handles root-specific patterns.
    """
    global compiled_file_patterns_global, compiled_dir_patterns_global
    global compiled_root_specific_file_patterns_global, compiled_root_specific_dir_patterns_global
    global DEBUG_EXCLUDE_ENABLED, DEBUG_TARGET_PATTERNS

    compiled_file_patterns_global = []
    compiled_dir_patterns_global = []
    compiled_root_specific_file_patterns_global = [] # Reset for each call, if any
    compiled_root_specific_dir_patterns_global = []  # Reset for each call, if any

    # Regex to detect root-specific patterns like "folder_name/file_or_pattern"
    # Captures "folder_name" in group 1 and "file_or_pattern" in group 2.
    # Ensures that there is at least one character in the remainder.
    root_specific_regex = re.compile(r'^(?P<root_name>[^/]+)/(?P<remainder>.+)$') # Changed `.*` to `.+`

    for original_glob in patterns_list:
        is_dir_pattern = original_glob.endswith('/')
        fnmatch_regex = fnmatch.translate(original_glob)

        # Check for root-specific pattern (e.g., toto/activate*)
        match_root_specific = root_specific_regex.match(original_glob)

        if original_glob.startswith('**/'):
            # Handle `**` patterns: make them match zero or more directories recursively.
            # Remove `(?s:` and `)\Z` from fnmatch.translate output to insert custom `(?:[^/]*/)*?`

            glob_after_wildcard = original_glob[3:] # removes `**` and the `/` if present

            # Translate only the part after `**`
            translated_suffix_inner = fnmatch.translate(glob_after_wildcard)
            # Remove `(?s:` and `)\Z` from this inner translation to make it flexible
            if translated_suffix_inner.startswith('(?s:') and translated_suffix_inner.endswith(')\\Z'):
                translated_suffix_inner = translated_suffix_inner[4:-3]

            # Construct the final regex: `^` (start) then `(?:[^/]*/)*?` (zero or more directories non-greedily)
            # then `translated_suffix_inner` and `\Z` (end)
            if is_dir_pattern:
                # For directories, ensure it matches the directory itself and anything inside it
                # Example: `**/dir/` -> matches `dir/` and `dir/file.txt`, `dir/subdir/`
                fnmatch_regex = r'(?s:(?:^|/)(?:[^/]*/)*?' + translated_suffix_inner + r'(?:.*)?)\Z'
                compiled_dir_patterns_global.append((original_glob, re.compile(fnmatch_regex)))
                if DEBUG_EXCLUDE_ENABLED and any(p in original_glob for p in DEBUG_TARGET_PATTERNS): # Adjusted filter check for compilation debug
                    print(f"DEBUG_REGEX_PROBLEM: (**/) DIR: '{original_glob}' -> Regex: '{fnmatch_regex}'")
            else: # ** pattern for files (e.g., **/.texlive*)
                # For files, match the filename directly anywhere.
                file_fnmatch_regex = r'(?s:(?:^|/)(?:[^/]*/)*?' + translated_suffix_inner + r')\Z'
                compiled_file_patterns_global.append((original_glob, re.compile(file_fnmatch_regex)))

                # For file patterns like `**/.texlive*`, they could also apply to directories like `.texlive2018/`
                # So, we also add a directory regex for them, similar to `**/dir/`
                dir_fnmatch_regex = r'(?s:(?:^|/)(?:[^/]*/)*?' + translated_suffix_inner + r'(?:.*)?)\Z'
                compiled_dir_patterns_global.append((original_glob, re.compile(dir_fnmatch_regex)))

                if DEBUG_EXCLUDE_ENABLED and any(p in original_glob for p in DEBUG_TARGET_PATTERNS): # Adjusted filter check for compilation debug
                    print(f"DEBUG_REGEX_PROBLEM: (**/) FILE/DIR: '{original_glob}' -> File Regex: '{file_fnmatch_regex}', Dir Regex: '{dir_fnmatch_regex}'")

        elif match_root_specific: #
            root_name = match_root_specific.group(1) # e.g., 'toto'
            remainder_glob = match_root_specific.group(2) # e.g., 'activate*' or '.config/'
            remainder_is_dir_pattern = remainder_glob.endswith('/')
            remainder_fnmatch_regex = fnmatch.translate(remainder_glob)

            # Remove (?s: and )\Z from the translated remainder for internal use
            if remainder_fnmatch_regex.startswith('(?s:') and remainder_fnmatch_regex.endswith(')\\Z'):
                remainder_fnmatch_regex_cleaned = remainder_fnmatch_regex[4:-3]
            else:
                remainder_fnmatch_regex_cleaned = remainder_fnmatch_regex

            # For directory remainders, ensure it matches the directory and anything inside it
            if remainder_is_dir_pattern:
                final_regex = r'(?s:' + remainder_fnmatch_regex_cleaned + r'(?:.*)?)\Z'
                compiled_root_specific_dir_patterns_global.append((root_name, re.compile(final_regex), original_glob))
                if DEBUG_EXCLUDE_ENABLED and any(p in original_glob for p in DEBUG_TARGET_PATTERNS): # Adjusted filter check for compilation debug
                     print(f"DEBUG_REGEX_PROBLEM: ROOT_SPECIFIC DIR: '{original_glob}' (root='{root_name}') -> Remainder Regex: '{final_regex}'")
            else: # File remainder
                final_regex = r'(?s:' + remainder_fnmatch_regex_cleaned + r')\Z'
                compiled_root_specific_file_patterns_global.append((root_name, re.compile(final_regex), original_glob))
                if DEBUG_EXCLUDE_ENABLED and any(p in original_glob for p in DEBUG_TARGET_PATTERNS): # Adjusted filter check for compilation debug
                    print(f"DEBUG_REGEX_PROBLEM: ROOT_SPECIFIC FILE: '{original_glob}' (root='{root_name}') -> Remainder Regex: '{final_regex}'")

        elif is_dir_pattern: # Normal directory pattern (e.g., my_dir/)
            # Ensure it matches the directory itself and anything inside it recursively.
            # If fnmatch.translate gives '(?s:dir/)\Z', we want '(?s:dir/(?:.*)?)\Z'
            # Check if fnmatch.translate already added recursive match for directories.
            if fnmatch_regex.endswith(')\\Z'):
                # Split at the last ')\Z' to insert '(?:.*)?' before it
                fnmatch_regex = fnmatch_regex.rsplit(')\\Z', 1)[0] + r'(?:.*)?)\Z'
            # If it doesn't end with ')\Z', it's already a simpler regex, just append `(?:.*)?`
            # (This else branch is less common for directory patterns from fnmatch.translate)
            else:
                fnmatch_regex = fnmatch_regex + r'(?:.*)?' # Fallback, should not happen often

            compiled_dir_patterns_global.append((original_glob, re.compile(fnmatch_regex)))
            if DEBUG_EXCLUDE_ENABLED and any(p in original_glob for p in DEBUG_TARGET_PATTERNS): # Adjusted filter check for compilation debug
                print(f"DEBUG_REGEX_PROBLEM: DIR (normal): '{original_glob}' -> Regex: '{fnmatch_regex}'")
        else: # Normal file pattern (e.g., my_file.txt)
            compiled_file_patterns_global.append((original_glob, re.compile(fnmatch_regex)))
            if DEBUG_EXCLUDE_ENABLED and any(p in original_glob for p in DEBUG_TARGET_PATTERNS): # Adjusted filter check for compilation debug
                print(f"DEBUG_REGEX_PROBLEM: FILE (normal): '{original_glob}' -> Regex: '{fnmatch_regex}'")

    return compiled_file_patterns_global, compiled_dir_patterns_global

def is_excluded(relative_path: str, is_dir_item: bool, current_scan_folder_basename: str = None, is_source_item: bool = True):
    """
    Checks if an item (file or directory) should be excluded based on compiled patterns.
    `relative_path` should be relative to the root of the scan (e.g., 'sub_dir/file.txt').
    `current_scan_folder_basename` is the basename of the current scan root (e.g., 'toto' for /home/user/toto).
    """
    global compiled_file_patterns_global, compiled_dir_patterns_global
    global compiled_root_specific_file_patterns_global, compiled_root_specific_dir_patterns_global
    global DEBUG_EXCLUDE_ENABLED, DEBUG_TARGET_PATTERNS

    # Normalize item_path for consistent matching (e.g., replace '\' with '/')
    normalized_item_path = relative_path.replace(os.sep, '/')

    # Activate verbose debug only if DEBUG_EXCLUDE_ENABLED is True and either
    # DEBUG_TARGET_PATTERNS is empty (debug all) or the path contains a target pattern.
    enable_verbose_debug = DEBUG_EXCLUDE_ENABLED and \
                           (not DEBUG_TARGET_PATTERNS or any(target_str in normalized_item_path for target_str in DEBUG_TARGET_PATTERNS))

    if enable_verbose_debug:
        print(f"\n--- DEBUG EXCLUDE ---")
        print(f"  Checking path: '{relative_path}' (Is Dir: {is_dir_item})")
        print(f"  Normalized path for regex: '{normalized_item_path}'")
        print(f"  Root Basename Context: '{current_scan_folder_basename}'")

    # 1. Check root-specific patterns first (e.g., 'toto/activate*')
    if current_scan_folder_basename: # Only proceed if root context is available
        if is_dir_item:
            for root_name, compiled_regex, original_glob in compiled_root_specific_dir_patterns_global:
                if root_name == current_scan_folder_basename:
                    # Match against the remainder of the path, relative to the root_name
                    # e.g., for 'toto/activate*', normalized_item_path='activate-smr2.csh' or 'subdir/config/'
                    if compiled_regex.match(normalized_item_path):
                        if enable_verbose_debug:
                            print(f"  -> EXCLUDED by ROOT_SPECIFIC_DIR pattern '{original_glob}' (regex '{compiled_regex.pattern}') on '{normalized_item_path}' for root '{current_scan_folder_basename}'")
                        return True, original_glob
        else: # is_file_item
            for root_name, compiled_regex, original_glob in compiled_root_specific_file_patterns_global:
                if root_name == current_scan_folder_basename:
                    # Match against the remainder of the path, relative to the root_name
                    if compiled_regex.match(normalized_item_path):
                        if enable_verbose_debug:
                            print(f"  -> EXCLUDED by ROOT_SPECIFIC_FILE pattern '{original_glob}' (regex '{compiled_regex.pattern}') on '{normalized_item_path}' for root '{current_scan_folder_basename}'")
                        return True, original_glob

    # 2. Check general patterns (starting with `**/` or general `filename`, `dir/`)
    if is_dir_item:
        for original_glob, compiled_regex in compiled_dir_patterns_global:
            if enable_verbose_debug:
                print(f"      ---> DEBUG: Checking against general DIR pattern: '{original_glob}'")
                print(f"      ---> DEBUG:   Compiled REGEX: '{compiled_regex.pattern}'")
                print(f"      ---> DEBUG:   Path to check: '{normalized_item_path}'")
                match_obj = compiled_regex.match(normalized_item_path) # Use match() for start of string
                print(f"      ---> DEBUG:   re.match result: {match_obj}")

            if compiled_regex.match(normalized_item_path):
                if enable_verbose_debug:
                    print(f"  -> EXCLUDED by general DIR pattern '{original_glob}' (regex '{compiled_regex.pattern}') on '{normalized_item_path}'")
                return True, original_glob
    else: # is_file_item
        for original_glob, compiled_regex in compiled_file_patterns_global:
            if enable_verbose_debug:
                print(f"      ---> DEBUG: Checking against general FILE pattern: '{original_glob}'")
                print(f"      ---> DEBUG:   Compiled REGEX: '{compiled_regex.pattern}'")
                print(f"      ---> DEBUG:   Path to check: '{normalized_item_path}'")
                match_obj = compiled_regex.match(normalized_item_path) # Use match() for start of string
                print(f"      ---> DEBUG:   re.match result: {match_obj}")

            if compiled_regex.match(normalized_item_path):
                if enable_verbose_debug:
                    print(f"  -> EXCLUDED by general FILE pattern '{original_glob}' (regex '{compiled_regex.pattern}') on '{normalized_item_path}'")
                return True, original_glob

    if enable_verbose_debug:
        print(f"  -> NOT EXCLUDED")
    return False, None

# --- Scanning and Comparison Logic ---

# Helper class to store scanned item details
class ScannedItem:
    def __init__(self, full_path, is_dir, size, mtime_ns, is_symlink, symlink_target=None, is_broken_symlink=False):
        self.full_path = full_path
        self.is_dir = is_dir
        self.is_file = not is_dir and not is_symlink # Explicitly store if it's a file
        self.size = size
        self.mtime_ns = mtime_ns # Use nanoseconds for precision, convert to seconds when needed
        self.is_symlink = is_symlink
        self.symlink_target = symlink_target # Stores the target path if it's a symlink
        self.is_broken_symlink = is_broken_symlink # Pass this directly from scan logic

def _scan_folder(root_path, comparison_results_dict, is_source_scan_flag, ignore_case):
    current_scan_folder_basename = os.path.basename(root_path)
    scanned_non_excluded_items = {}
    print(f"{'Scanning folder':<25} : {root_path} ...")

    for dirpath, dirnames, filenames in os.walk(root_path, followlinks=False):
        current_relative_dir = os.path.relpath(dirpath, root_path)
        if current_relative_dir == ".":
            current_relative_dir = ""

        # --- Process directories ---
        dirnames_to_process = list(dirnames)
        for i in reversed(range(len(dirnames_to_process))): # Iterate in reverse to allow removal
            dirname = dirnames_to_process[i]
            full_path = os.path.join(dirpath, dirname)
            relative_path = os.path.join(current_relative_dir, dirname)
            relative_path_with_sep = relative_path + os.sep

            # Exclusion check for directories
            is_excluded_flag, matched_pattern = is_excluded(
                relative_path_with_sep, True, current_scan_folder_basename, is_source_scan_flag
            )

            if is_excluded_flag:
                # Add to EXCLUDED category directly as a ComparisonItem
                comp_item = ComparisonItem(relative_path)
                if is_source_scan_flag:
                    comp_item.is_excluded_source = True
                    comp_item.exists_source = True
                    comp_item.is_dir_source = True
                else:
                    comp_item.is_excluded_target = True
                    comp_item.exists_target = True
                    comp_item.is_dir_target = True
                comp_item.exclusion_matched_pattern = matched_pattern
                comp_item.category_key = REPORT_EXCLUDED # Assign its category
                comparison_results_dict[REPORT_EXCLUDED].append(comp_item)
                dirnames.remove(dirname) # Prune directory from os.walk
                continue

            # Check for broken symlinks (directories can also be symlinks)
            is_current_symlink = os.path.islink(full_path)
            is_broken_symlink = False
            symlink_target = None
            if is_current_symlink:
                symlink_target = os.path.realpath(full_path)
                is_broken_symlink = not os.path.exists(symlink_target)
                if is_broken_symlink:
                    # Add to BROKEN_SYMLINK category directly
                    comp_item = ComparisonItem(relative_path)
                    try:
                        symlink_stat = os.lstat(full_path)
                        item_size = symlink_stat.st_size
                    except OSError as e:
                        item_size = 0 # Cannot get size, set to 0
                        comp_item.scan_error_source = f"Could not stat symlink: {e}" if is_source_scan_flag else None
                        comp_item.scan_error_target = f"Could not stat symlink: {e}" if not is_source_scan_flag else None

                    if is_source_scan_flag:
                        comp_item.is_broken_symlink_source = True
                        comp_item.exists_source = True
                        comp_item.is_dir_source = os.path.isdir(full_path) # Check if link points to a dir (even if broken)
                        comp_item.is_file_source = os.path.isfile(full_path)
                        comp_item.is_symlink_source = True
                        comp_item.symlink_target_source = symlink_target
                        comp_item.size_source = item_size
                        comp_item.mtime_ns_source = symlink_stat.st_mtime_ns if 'symlink_stat' in locals() else None
                        comp_item.category_key = REPORT_BROKEN_SYMLINK_SOURCE
                        comparison_results_dict[REPORT_BROKEN_SYMLINK_SOURCE].append(comp_item)
                    else:
                        comp_item.is_broken_symlink_target = True
                        comp_item.exists_target = True
                        comp_item.is_dir_target = os.path.isdir(full_path)
                        comp_item.is_file_target = os.path.isfile(full_path)
                        comp_item.is_symlink_target = True
                        comp_item.symlink_target_target = symlink_target
                        comp_item.size_target = item_size
                        comp_item.mtime_ns_target = symlink_stat.st_mtime_ns if 'symlink_stat' in locals() else None
                        comp_item.category_key = REPORT_BROKEN_SYMLINK_TARGET
                        comparison_results_dict[REPORT_BROKEN_SYMLINK_TARGET].append(comp_item)

                    dirnames.remove(dirname) # Prune broken symlink directory from os.walk
                    continue

            # If not excluded and not a broken symlink, add directory to scanned items
            try:
                # Use os.lstat for symlinks, os.stat for real directories
                dir_stat = os.lstat(full_path) if is_current_symlink else os.stat(full_path)
                key_path = relative_path.lower() if ignore_case else relative_path
                scanned_non_excluded_items[key_path] = ScannedItem(
                    full_path=full_path,
                    is_dir=True, # It's a directory (or a symlink pointing to a directory that exists)
                    size=dir_stat.st_size, # Size of the symlink itself if it is a symlink
                    mtime_ns=dir_stat.st_mtime_ns,
                    is_symlink=is_current_symlink,
                    symlink_target=symlink_target,
                    is_broken_symlink=is_broken_symlink
                )
            except OSError as e:
                comp_item_error = ComparisonItem(relative_path)
                if is_source_scan_flag:
                    comp_item_error.exists_source = True
                    comp_item_error.is_dir_source = True
                    comp_item_error.scan_error_source = f"Directory scanning error: {e}"
                    comp_item_error.category_key = REPORT_COMPARE_ERROR
                    comparison_results_dict[REPORT_COMPARE_ERROR].append(comp_item_error)
                else:
                    comp_item_error.exists_target = True
                    comp_item_error.is_dir_target = True
                    comp_item_error.scan_error_target = f"Directory scanning error: {e}"
                    comp_item_error.category_key = REPORT_COMPARE_ERROR
                    comparison_results_dict[REPORT_COMPARE_ERROR].append(comp_item_error)
                dirnames.remove(dirname) # Can't process, remove
                continue

        # --- Process files ---
        for filename in filenames:
            full_path = os.path.join(dirpath, filename)
            relative_path = os.path.join(current_relative_dir, filename)

            # Exclusion check for files
            is_excluded_flag, matched_pattern = is_excluded(
                relative_path, False, current_scan_folder_basename, is_source_scan_flag
            )

            if is_excluded_flag:
                # Add to EXCLUDED category directly as a ComparisonItem
                comp_item = ComparisonItem(relative_path)
                try: # Try to get size even for excluded files
                    file_stat = os.lstat(full_path) if os.path.islink(full_path) else os.stat(full_path)
                    item_size = file_stat.st_size
                    item_mtime_ns = file_stat.st_mtime_ns
                except OSError:
                    item_size = None
                    item_mtime_ns = None

                if is_source_scan_flag:
                    comp_item.is_excluded_source = True
                    comp_item.exists_source = True
                    comp_item.is_file_source = True # Assume file unless it's a symlink
                    comp_item.size_source = item_size
                    comp_item.mtime_ns_source = item_mtime_ns
                else:
                    comp_item.is_excluded_target = True
                    comp_item.exists_target = True
                    comp_item.is_file_target = True
                    comp_item.size_target = item_size
                    comp_item.mtime_ns_target = item_mtime_ns
                comp_item.exclusion_matched_pattern = matched_pattern
                comp_item.category_key = REPORT_EXCLUDED # Assign its category
                comparison_results_dict[REPORT_EXCLUDED].append(comp_item)
                continue # Skip further processing for excluded files

            # Check for broken symlinks (files can also be symlinks)
            is_current_symlink = os.path.islink(full_path)
            is_broken_symlink = False
            symlink_target = None
            if is_current_symlink:
                symlink_target = os.path.realpath(full_path)
                is_broken_symlink = not os.path.exists(symlink_target)
                if is_broken_symlink:
                    # Add to BROKEN_SYMLINK category directly
                    comp_item = ComparisonItem(relative_path)
                    try:
                        symlink_stat = os.lstat(full_path)
                        item_size = symlink_stat.st_size
                    except OSError as e:
                        item_size = 0
                        comp_item.scan_error_source = f"Could not stat symlink: {e}" if is_source_scan_flag else None
                        comp_item.scan_error_target = f"Could not stat symlink: {e}" if not is_source_scan_flag else None

                    if is_source_scan_flag:
                        comp_item.is_broken_symlink_source = True
                        comp_item.exists_source = True
                        comp_item.is_file_source = os.path.isfile(full_path) # Check if link points to a file (even if broken)
                        comp_item.is_dir_source = os.path.isdir(full_path)
                        comp_item.is_symlink_source = True
                        comp_item.symlink_target_source = symlink_target
                        comp_item.size_source = item_size
                        comp_item.mtime_ns_source = symlink_stat.st_mtime_ns if 'symlink_stat' in locals() else None
                        comp_item.category_key = REPORT_BROKEN_SYMLINK_SOURCE
                        comparison_results_dict[REPORT_BROKEN_SYMLINK_SOURCE].append(comp_item)
                    else:
                        comp_item.is_broken_symlink_target = True
                        comp_item.exists_target = True
                        comp_item.is_file_target = os.path.isfile(full_path)
                        comp_item.is_dir_target = os.path.isdir(full_path)
                        comp_item.is_symlink_target = True
                        comp_item.symlink_target_target = symlink_target
                        comp_item.size_target = item_size
                        comp_item.mtime_ns_target = symlink_stat.st_mtime_ns if 'symlink_stat' in locals() else None
                        comp_item.category_key = REPORT_BROKEN_SYMLINK_TARGET
                        comparison_results_dict[REPORT_BROKEN_SYMLINK_TARGET].append(comp_item)
                    continue # Skip further processing for broken symlinks

            # If not excluded and not a broken symlink, add file to scanned items
            try:
                file_stat = os.lstat(full_path) if is_current_symlink else os.stat(full_path)
                key_path = relative_path.lower() if ignore_case else relative_path
                scanned_non_excluded_items[key_path] = ScannedItem(
                    full_path=full_path,
                    is_dir=False,
                    size=file_stat.st_size,
                    mtime_ns=file_stat.st_mtime_ns,
                    is_symlink=is_current_symlink,
                    symlink_target=symlink_target,
                    is_broken_symlink=is_broken_symlink # Should be False here if not "broken"
                )
            except OSError as e:
                comp_item_error = ComparisonItem(relative_path)
                if is_source_scan_flag:
                    comp_item_error.exists_source = True
                    comp_item_error.is_file_source = True
                    comp_item_error.scan_error_source = f"File scanning error: {e}"
                    comp_item_error.category_key = REPORT_COMPARE_ERROR
                    comparison_results_dict[REPORT_COMPARE_ERROR].append(comp_item_error)
                else:
                    comp_item_error.exists_target = True
                    comp_item_error.is_file_target = True
                    comp_item_error.scan_error_target = f"File scanning error: {e}"
                    comp_item_error.category_key = REPORT_COMPARE_ERROR
                    comparison_results_dict[REPORT_COMPARE_ERROR].append(comp_item_error)
                continue # Skip further processing for files with scan errors

    print("Finished scanning folders\\n")
    return scanned_non_excluded_items


# Main comparison function
def compare_scanned_items(source_items: dict, target_items: dict, comparison_results_dict: dict,
                          time_tolerance_seconds: float, hash_algorithm: Optional[str] = None):

    print("Comparing items ...")

    all_relative_paths = sorted(list(source_items.keys() | target_items.keys()))
    hash_check_enabled = hash_algorithm is not None

    for relative_path in all_relative_paths:
        source_scanned_item = source_items.get(relative_path)
        target_scanned_item = target_items.get(relative_path)

        comp_item = ComparisonItem(relative_path)

        # Populate source-specific attributes from ScannedItem
        if source_scanned_item:
            comp_item.exists_source = True
            comp_item.is_dir_source = source_scanned_item.is_dir
            comp_item.is_file_source = source_scanned_item.is_file
            comp_item.is_symlink_source = source_scanned_item.is_symlink
            comp_item.size_source = source_scanned_item.size
            comp_item.mtime_ns_source = source_scanned_item.mtime_ns
            comp_item.symlink_target_source = source_scanned_item.symlink_target
            comp_item.is_broken_symlink_source = source_scanned_item.is_broken_symlink
            # Hash is calculated only if needed for comparison later, not during initial scan
            # comp_item.hash_source = ... (will be calculated here if hash_check is on)

        # Populate target-specific attributes from ScannedItem
        if target_scanned_item:
            comp_item.exists_target = True
            comp_item.is_dir_target = target_scanned_item.is_dir
            comp_item.is_file_target = target_scanned_item.is_file
            comp_item.is_symlink_target = target_scanned_item.is_symlink
            comp_item.size_target = target_scanned_item.size
            comp_item.mtime_ns_target = target_scanned_item.mtime_ns
            comp_item.symlink_target_target = target_scanned_item.symlink_target
            comp_item.is_broken_symlink_target = target_scanned_item.is_broken_symlink
            # comp_item.hash_target = ... (will be calculated here if hash_check is on)


        # --- Classification Logic ---

        # 1. Items only in Source
        if comp_item.exists_source and not comp_item.exists_target:
            if comp_item.is_dir_source:
                comp_item.category_key = REPORT_ONLY_IN_SOURCE_DIR
            else: # File or Symlink
                comp_item.category_key = REPORT_ONLY_IN_SOURCE_FILE
            comparison_results_dict[comp_item.category_key].append(comp_item)
            continue # Go to next item

        # 2. Items only in Target
        if not comp_item.exists_source and comp_item.exists_target:
            if comp_item.is_dir_target:
                comp_item.category_key = REPORT_ONLY_IN_TARGET_DIR
            else: # File or Symlink
                comp_item.category_key = REPORT_ONLY_IN_TARGET_FILE
            comparison_results_dict[comp_item.category_key].append(comp_item)
            continue # Go to next item

        # From here, item exists in BOTH source and target
        # Check for type mismatch (e.g., file in source, dir in target)
        if comp_item.is_dir_source != comp_item.is_dir_target or \
           comp_item.is_file_source != comp_item.is_file_target or \
           comp_item.is_symlink_source != comp_item.is_symlink_target:
            comp_item.type_mismatch_description = (
                f"Source: {'Dir' if comp_item.is_dir_source else 'File' if comp_item.is_file_source else 'Symlink' if comp_item.is_symlink_source else 'Unknown'} "
                f"vs Target: {'Dir' if comp_item.is_dir_target else 'File' if comp_item.is_file_target else 'Symlink' if comp_item.is_symlink_target else 'Unknown'}"
            )
            comp_item.category_key = REPORT_DIFFERENT_TYPE
            comparison_results_dict[REPORT_DIFFERENT_TYPE].append(comp_item)
            continue # Go to next item

        # Handle directories first if they match type
        if comp_item.is_dir_source and comp_item.is_dir_target:
            # Directories are considered identical if they exist in both places with matching types
            comp_item.category_key = REPORT_IDENTICAL_DIR # New category for identical directories
            comparison_results_dict[REPORT_IDENTICAL_DIR].append(comp_item)
            continue # Go to next item


        # From here, we only compare files and symlinks that match type (both are files, or both are symlinks)
        # Note: Symlink content is not hashed, only the target path and existence.
        # If symlinks point to different targets, they are "different".
        # If symlinks point to same target, they are "identical".
        if comp_item.is_symlink_source and comp_item.is_symlink_target:
            if comp_item.symlink_target_source == comp_item.symlink_target_target:
                comp_item.category_key = REPORT_IDENTICAL_SYMLINK # New category for identical symlinks
                comparison_results_dict[REPORT_IDENTICAL_SYMLINK].append(comp_item)
            else:
                # Symlinks exist, types match, but targets differ
                comp_item.category_key = REPORT_DIFFERENT_SYMLINK_TARGET # New category
                comparison_results_dict[REPORT_DIFFERENT_SYMLINK_TARGET].append(comp_item)
            continue # Go to next item


        # From here, we only compare regular files (both are_file_source and is_file_target are True)
        if not comp_item.is_file_source or not comp_item.is_file_target:
            # This should ideally not be reached if type mismatch is handled above,
            # but as a safeguard.
            comp_item.comparison_error_message = "Internal logic error: Comparing non-file types as files."
            comp_item.category_key = REPORT_COMPARE_ERROR
            comparison_results_dict[REPORT_COMPARE_ERROR].append(comp_item)
            continue


        # --- File Comparison Logic (for actual files) ---
        is_identical_size = (comp_item.size_source == comp_item.size_target)

        # Check mtime tolerance. Convert ns to seconds for comparison.
        mtime_source_sec = comp_item.mtime_ns_source / 1e9 if comp_item.mtime_ns_source is not None else None
        mtime_target_sec = comp_item.mtime_ns_target / 1e9 if comp_item.mtime_ns_target is not None else None

        is_identical_mtime = False
        if mtime_source_sec is not None and mtime_target_sec is not None:
            is_identical_mtime = abs(mtime_source_sec - mtime_target_sec) <= time_tolerance_seconds
        elif mtime_source_sec is None and mtime_target_sec is None:
            is_identical_mtime = True # Both missing mtime, treat as identical for this aspect
        # If one mtime is None and the other is not, is_identical_mtime remains False

        # Assume identical, then check for differences
        is_content_identical = False

        if is_identical_size and is_identical_mtime:
            if hash_check_enabled:
                try:
                    # Calculate hash only if needed and not already calculated
                    if comp_item.hash_source is None:
                        comp_item.hash_source = calculate_hash(source_scanned_item.full_path, hash_algorithm)
                    if comp_item.hash_target is None:
                        comp_item.hash_target = calculate_hash(target_scanned_item.full_path, hash_algorithm)

                    if comp_item.hash_source is None or comp_item.hash_target is None:
                        comp_item.comparison_error_message = "Failed to calculate hash for one or both files."
                        comp_item.category_key = REPORT_COMPARE_ERROR
                        comparison_results_dict[REPORT_COMPARE_ERROR].append(comp_item)
                        continue

                    if comp_item.hash_source == comp_item.hash_target:
                        is_content_identical = True
                    # Else: Hashes differ, so content is different (fall through to different categories)
                except Exception as e:
                    comp_item.comparison_error_message = f"Hash calculation error: {e}"
                    comp_item.category_key = REPORT_COMPARE_ERROR
                    comparison_results_dict[REPORT_COMPARE_ERROR].append(comp_item)
                    continue
            else: # No hash check, so if size and mtime are identical, consider content identical
                is_content_identical = True

        if is_content_identical:
            comp_item.category_key = REPORT_IDENTICAL
            comparison_results_dict[REPORT_IDENTICAL].append(comp_item)
        else:
            # If not identical, then it's different. Determine which is newer.
            # This logic is reached if:
            # 1. Sizes differ (is_identical_size is False)
            # 2. Sizes are identical, but mtimes differ significantly (is_identical_mtime is False)
            # 3. Sizes and mtimes are identical, but hash_check is enabled and hashes differ.
            if mtime_source_sec is not None and mtime_target_sec is not None:
                if mtime_source_sec > mtime_target_sec:
                    comp_item.category_key = REPORT_DIFFERENT_CONTENT_SOURCE_NEWER
                    comparison_results_dict[REPORT_DIFFERENT_CONTENT_SOURCE_NEWER].append(comp_item)
                elif mtime_target_sec > mtime_source_sec:
                    comp_item.category_key = REPORT_DIFFERENT_CONTENT_TARGET_NEWER
                    comparison_results_dict[REPORT_DIFFERENT_CONTENT_TARGET_NEWER].append(comp_item)
                else: # mtimes are identical, but sizes or hashes differ (if hash check enabled)
                    # This implies size mismatch OR (size match AND mtime match AND hash mismatch)
                    comp_item.comparison_error_message = f"File mismatch (size or hash) with identical mtimes: {comp_item.size_source} vs {comp_item.size_target}"
                    comp_item.category_key = REPORT_COMPARE_ERROR
                    comparison_results_dict[REPORT_COMPARE_ERROR].append(comp_item)
            else: # One or both mtimes are None, cannot determine newer based on mtime
                comp_item.comparison_error_message = "Cannot determine newer file due to missing modification time(s)."
                comp_item.category_key = REPORT_COMPARE_ERROR
                comparison_results_dict[REPORT_COMPARE_ERROR].append(comp_item)

    print("Finished comparing items")
    return comparison_results_dict


# --- Argument Parsing Function ---
def getArguments():
    """
    Sets up and parses command-line arguments for the folder comparison tool.

    Returns:
        argparse.Namespace: An object containing the parsed arguments.
    """
    WRAP_WIDTH = 78 

    main_description_raw = "Compares folders based on existence, modification dates, size, and optionally SHA256/MD5 hash, generating a detailed report."
    main_description_wrapped = textwrap.fill(main_description_raw, width=WRAP_WIDTH)

    comparison_logic_raw = """
File Comparison Logic:
1. Initial Size Comparison:
   - If file sizes differ, files are immediately classified as "DIFFERENT_CONTENT".
     They are then differentiated by "SOURCE is newer" or "TARGET is newer" based on their modification time (mtime).
2. Modification Time (mtime) Comparison (if sizes are identical):
   - If abs(source_mtime - target_mtime) <= --time-tolerance (mtime within tolerance):
     - If --hash-check is NOT specified: Files are classified as "IDENTICAL".
     - If --hash-check IS specified: A hash check is performed.
       - If hashes are identical: Files are classified as "IDENTICAL".
       - If hashes are different: Files are classified as "DIFFERENT_CONTENT" (and differentiated by mtime).
   - If abs(source_mtime - target_mtime) > --time-tolerance (mtime OUTSIDE tolerance):
     - Files are classified as "DIFFERENT_CONTENT" (and differentiated by mtime).
     - A hash check is NOT performed, as the mtime difference is sufficient to mark them as different.
"""
    
    wrapped_comparison_logic_lines = []
    for line in comparison_logic_raw.splitlines():
        if not line.strip():
            wrapped_comparison_logic_lines.append("")
            continue
        
        leading_spaces = len(line) - len(line.lstrip())
        indent = " " * leading_spaces
        content = line.lstrip()

        wrapped_content = textwrap.fill(content, 
                                        width=WRAP_WIDTH - leading_spaces, 
                                        initial_indent=indent, 
                                        subsequent_indent=indent)
        wrapped_comparison_logic_lines.append(wrapped_content)
    
    full_description_with_logic = main_description_wrapped + "\n" + "\n".join(wrapped_comparison_logic_lines)

    # Custom usage string with '--' and specific formatting
    custom_usage_string_formatted = (
        'usage: %(prog)s [-h] [--ignore-case]\n'
        '                                  [--exclude [EXCLUDE [EXCLUDE ...]]]\n'
        '                                  [--exclude-file EXCLUDE_FILE]\n'
        '                                  [--time-tolerance TIME_TOLERANCE in second]\n'
        '                                  [--hash-check [{md5,sha256}]]\n'
        '                                  [--debug-exclude]\n'
        '                                  [--debug-exclude-filter-patterns [DEBUG_EXCLUDE_FILTER_PATTERNS [DEBUG_EXCLUDE_FILTER_PATTERNS ...]]]\n'
        '                                  -- source_folder target_folder'
    )

    parser = argparse.ArgumentParser(
        usage=custom_usage_string_formatted, 
        description=full_description_with_logic,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=None
    )

    parser.add_argument("source_folder", help="The path to the source folder.")
    parser.add_argument("target_folder", help="The path to the target folder.")
    parser.add_argument("--ignore-case", action="store_true", help="Perform case-insensitive comparison of file paths.")
    parser.add_argument("--exclude", nargs='*', default=[],
                        help="One or more shell-style patterns (e.g., '*.tmp' 'project/temp*' 'cache/') to exclude files or folders. "
                             "Directory patterns should end with a slash.")
    parser.add_argument("--exclude-file", help="Path to a file containing shell-style patterns to exclude (one pattern per line, # for comments).")
    parser.add_argument("--time-tolerance", type=float, default=1.0,
                        help="Maximum time difference in seconds (e.g., 0.001 for 1ms) "
                             "for two file modification timestamps to be considered IDENTICAL, along with identical size. "
                             "Default is 1.0. This option is used as a primary filter before hash check. "
                             "If files have identical size but their timestamps are outside this tolerance, "
                             "they are considered DIFFERENT, even if --hash-check is enabled. "
                             "Note: Sometimes, when manipulating files, the modification timestamp may be "
                             "rounded to the nearest second. Using --time-tolerance 1.0 (the default) is "
                             "recommended to treat such files as identical if their size and (optionally) hash also match.")
    parser.add_argument("--hash-check", choices=['md5', 'sha256'], nargs='?', const='md5',
                        help="If enabled, perform a content hash comparison for files "
                             "that have identical sizes and timestamps within time-tolerance to confirm identity "
                             "more rigorously. Specify 'md5' or 'sha256' as the algorithm. "
                             "Defaults to MD5 if no algorithm is specified. "
                             "The hash check is performed ONLY if sizes are identical AND "
                             "timestamps are within the specified time-tolerance.")
    parser.add_argument("--debug-exclude", action="store_true", help="Enable verbose debugging output for exclusion pattern matching.")
    parser.add_argument("--debug-exclude-filter-patterns", nargs='*', default=[],
                        help="When --debug-exclude is enabled, only print debug messages for paths containing one of these substring patterns (e.g., '.aienv*' '.texlive*').")

    return parser.parse_args()


# --- Formatin mtime Function ---
def format_mtime_ns(mtime_ns: Optional[int]) -> str:
    """
    Formats a modification time in nanoseconds to a human-readable string.
    Returns 'N/A' if mtime_ns is None.
    """
    if mtime_ns is None:
        return "N/A"
    try:
        # Convert nanoseconds to seconds for datetime.fromtimestamp
        mtime_sec = mtime_ns / 1e9
        return datetime.datetime.fromtimestamp(mtime_sec).strftime('%Y-%m-%d %H:%M:%S')
    except (TypeError, ValueError):
        return "Invalid Date"


# --- Formatin date Function ---
def get_formatted_dates():
    """
    Generates and returns formatted local and UTC date strings.

    Returns:
        tuple: A tuple containing (local_date_str, utc_date_str).
    """
    now_local = datetime.datetime.now()
    now_utc = datetime.datetime.utcnow()

    # Calculate UTC offset for local time
    utc_offset_seconds = time.altzone if time.daylight else time.timezone
    utc_offset_hours = -utc_offset_seconds / 3600.0

    if utc_offset_hours == int(utc_offset_hours):
        local_offset_str = f"UTC{'+' if utc_offset_hours >= 0 else ''}{int(utc_offset_hours)}"
    else:
        sign = '+' if utc_offset_hours >= 0 else ''
        hours = int(abs(utc_offset_hours))
        minutes = int((abs(utc_offset_hours) - hours) * 60)
        local_offset_str = f"UTC{sign}{hours:02d}:{minutes:02d}"

    local_date_str = now_local.strftime(f"%a %b %d %H:%M:%S {local_offset_str} %Y")
    utc_date_str = now_utc.strftime("%a %b %d %H:%M:%S UTC %Y")

    return local_date_str, utc_date_str

def format_size_human_readable(size_in_bytes):
    """
    Converts a size in bytes to a human-readable format (B, KB, MB, GB, TB).
    Returns "None bytes" if size_bytes is None.

    Exemples d'utilisation:
    print(format_size_human_readable(0))                   # 0    Bytes
    print(format_size_human_readable(100))                 # 100  Bytes
    print(format_size_human_readable(1023))                # 1023 Bytes
    print(format_size_human_readable(1024))                # 1.0  KB
    print(format_size_human_readable(1500))                # 1.46 KB
    print(format_size_human_readable(1024 * 1024))         # 1.0  MB
    print(format_size_human_readable(5 * (1024**3)))       # 5.0  GB
    print(format_size_human_readable(2.5 * (1024**4)))     # 2.5  TB
    print(format_size_human_readable(1234567890))          # 1.15 GB
    print(format_size_human_readable(9876543210987654321)) # 8.59 EB (Exabytes)
    """
    if size_in_bytes is None:
        return "None bytes"

    if size_in_bytes == 0:
        return "0 Bytes"

    if size_in_bytes < 0:
        PM=-1.
        size_in_bytes = size_in_bytes * PM
    else:
        PM=1.

    # List of units : Bytes, Kilobytes, Megabytes, Gigabytes, Terabytes, Petabytes, etc.
    units = ['Bytes', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB']

    # Calculates the index of the appropriate unit
    # Ex: for 1024 Bytes, log base 1024 of 1024 is 1, so KB (index 1)
    # For 1023 Bytes, log base 1024 of 1023 is 0.99..., floor gives 0, so Bytes (index 0)
    i = int(math.floor(math.log(size_in_bytes, 1024)))

    # Ensure that the index does not exceed the number of units available
    if i >= len(units):
        i = len(units) - 1 # Use the largest unit available

    # Calculates the value in the chosen unit
    power = 1024 ** i
    formatted_value = PM*round(size_in_bytes / power, 2) # Rounded to 2 decimal places
    size_in_bytes = size_in_bytes * PM

    return f"{formatted_value} {units[i]}"


def _print_category_details(
    category_key: str,
    items: list,
    source_items_scanned: dict, # For potential debug/cross-ref, but less used directly
    target_items_scanned: dict, # For potential debug/cross-ref, but less used directly
    source_folder_cleaned: str,
    target_folder_cleaned: str,
    hash_check: str,
    category_number: int
):
    current_dirs = 0
    current_files = 0
    current_symlinks = 0 # Count symlinks
    total_items_in_category = 0
    total_dirs_in_category = 0
    total_files_in_category = 0
    total_symlinks_in_category = 0 # Count symlinks in category
    current_size = 0 # Accumulates total_size for the current category

    # Specific totals for categories 6 and 7 for net change calculation
    temp_target_older_size_cat6 = 0 # Total of target_item.size for REPORT_DIFFERENT_CONTENT_SOURCE_NEWER
    temp_source_older_size_cat7 = 0 # Total of source_item.size for REPORT_DIFFERENT_CONTENT_TARGET_NEWER

    # --- Step 1: Calculate totals by iterating through items ---
    for comp_item in items:
        total_items_in_category += 1
        # Determine if it's a directory, file or symlink from its actual type in one of the folders
        # This is more robust as a comp_item might be only in source/target
        is_dir = comp_item.is_dir_source or comp_item.is_dir_target
        is_file = comp_item.is_file_source or comp_item.is_file_target
        is_symlink = comp_item.is_symlink_source or comp_item.is_symlink_target

        if is_dir:
            total_dirs_in_category += 1
        elif is_file:
            total_files_in_category += 1
        elif is_symlink: # Treat symlinks as a separate count
            total_symlinks_in_category += 1

        # Accumulate size based on category
        if category_key in [REPORT_ONLY_IN_SOURCE_FILE, REPORT_ONLY_IN_SOURCE_DIR, REPORT_BROKEN_SYMLINK_SOURCE]:
            if comp_item.size_source is not None:
                current_size += comp_item.size_source
        elif category_key in [REPORT_ONLY_IN_TARGET_FILE, REPORT_ONLY_IN_TARGET_DIR, REPORT_BROKEN_SYMLINK_TARGET]:
            if comp_item.size_target is not None:
                current_size += comp_item.size_target
        elif category_key in [REPORT_IDENTICAL, REPORT_IDENTICAL_DIR, REPORT_IDENTICAL_SYMLINK, REPORT_DIFFERENT_TYPE, REPORT_DIFFERENT_SYMLINK_TARGET, REPORT_COMPARE_ERROR]:
            # For items existing in both, prioritize source size for total, or target if source missing
            if comp_item.size_source is not None:
                current_size += comp_item.size_source
            elif comp_item.size_target is not None:
                current_size += comp_item.size_target
        elif category_key == REPORT_DIFFERENT_CONTENT_SOURCE_NEWER:
            if comp_item.size_source is not None:
                current_size += comp_item.size_source
            if comp_item.size_target is not None: # For net change calculation
                temp_target_older_size_cat6 += comp_item.size_target
        elif category_key == REPORT_DIFFERENT_CONTENT_TARGET_NEWER:
            if comp_item.size_target is not None:
                current_size += comp_item.size_target
            if comp_item.size_source is not None: # For net change calculation
                temp_source_older_size_cat7 += comp_item.size_source
        elif category_key == REPORT_EXCLUDED:
            # Excluded items size: take source if excluded from source, else target
            if comp_item.is_excluded_source and comp_item.size_source is not None:
                current_size += comp_item.size_source
            elif comp_item.is_excluded_target and comp_item.size_target is not None:
                current_size += comp_item.size_target

    # --- Step 2: Print totals ---
    print(f"    Total items: {total_items_in_category}")
    if total_files_in_category > 0:
        print(f"      - Files: {total_files_in_category}")
    if total_dirs_in_category > 0:
        print(f"      - Folders: {total_dirs_in_category}")
    if total_symlinks_in_category > 0: # Print symlink count
        print(f"      - Symbolic Links: {total_symlinks_in_category}")

    print(f"    Total size: {format_size_human_readable(current_size)}")

    if category_key == REPORT_DIFFERENT_CONTENT_SOURCE_NEWER:
        net_change_bytes = current_size - temp_target_older_size_cat6
        print(f"    Net disk space change if SOURCE replaces TARGET: {format_size_human_readable(net_change_bytes)} ({'+' if net_change_bytes >= 0 else ''}{net_change_bytes} bytes)")
    elif category_key == REPORT_DIFFERENT_CONTENT_TARGET_NEWER:
        net_change_bytes = current_size - temp_source_older_size_cat7
        print(f"    Net disk space change if TARGET replaces SOURCE: {format_size_human_readable(net_change_bytes)} ({'+' if net_change_bytes >= 0 else ''}{net_change_bytes} bytes)")

    # --- Step 3: Print individual item details if not too many ---
    if total_items_in_category > 0 and total_items_in_category <= 20: # Limit detail output
        print(f"    Details:")
        for comp_item in items:
            item_path = comp_item.relative_path
            mtime_source_str = format_mtime_ns(comp_item.mtime_ns_source)
            mtime_target_str = format_mtime_ns(comp_item.mtime_ns_target)

            if category_key in [REPORT_IDENTICAL, REPORT_IDENTICAL_DIR, REPORT_IDENTICAL_SYMLINK]:
                size_info = ""
                if comp_item.is_file_source: # Only show size for files
                    size_info = f" (size: {format_size_human_readable(comp_item.size_source)})"
                elif comp_item.is_symlink_source: # Show symlink target
                    size_info = f" (target: '{comp_item.symlink_target_source}')"
                print(f"      {item_path}{size_info}")

            elif category_key == REPORT_DIFFERENT_CONTENT_SOURCE_NEWER:
                print(f"      {item_path}")
                print(f"        Source: Size: {format_size_human_readable(comp_item.size_source)}, MTime: {mtime_source_str}, Hash: {comp_item.hash_source if hash_check else 'N/A'}")
                print(f"        Target: Size: {format_size_human_readable(comp_item.size_target)}, MTime: {mtime_target_str}, Hash: {comp_item.hash_target if hash_check else 'N/A'}")
            elif category_key == REPORT_DIFFERENT_CONTENT_TARGET_NEWER:
                print(f"      {item_path}")
                print(f"        Source: Size: {format_size_human_readable(comp_item.size_source)}, MTime: {mtime_source_str}, Hash: {comp_item.hash_source if hash_check else 'N/A'}")
                print(f"        Target: Size: {format_size_human_readable(comp_item.size_target)}, MTime: {mtime_target_str}, Hash: {comp_item.hash_target if hash_check else 'N/A'}")
            elif category_key == REPORT_ONLY_IN_SOURCE_FILE:
                print(f"      {item_path} (size: {format_size_human_readable(comp_item.size_source)}, mtime: {mtime_source_str})")
            elif category_key == REPORT_ONLY_IN_SOURCE_DIR:
                 print(f"      {item_path}")
            elif category_key == REPORT_ONLY_IN_TARGET_FILE:
                print(f"      {item_path} (size: {format_size_human_readable(comp_item.size_target)}, mtime: {mtime_target_str})")
            elif category_key == REPORT_ONLY_IN_TARGET_DIR:
                print(f"      {item_path}")
            elif category_key == REPORT_EXCLUDED:
                # Decide which size/mtime to display based on exclusion origin
                item_size = comp_item.size_source if comp_item.is_excluded_source else comp_item.size_target
                item_mtime_ns = comp_item.mtime_ns_source if comp_item.is_excluded_source else comp_item.mtime_ns_target
                item_mtime_str = format_mtime_ns(item_mtime_ns)
                matched_pattern = comp_item.exclusion_matched_pattern
                exclusion_origin = "source" if comp_item.is_excluded_source else "target"
                print(f"      {item_path} ({exclusion_origin} size: {format_size_human_readable(item_size)}, mtime: {item_mtime_str}, matched by '{matched_pattern}')")
            elif category_key == REPORT_DIFFERENT_TYPE:
                type_desc = comp_item.type_mismatch_description
                print(f"      {item_path} ({type_desc})")
                # Add more details if available
                if comp_item.exists_source:
                    src_type = "Dir" if comp_item.is_dir_source else "File" if comp_item.is_file_source else "Symlink" if comp_item.is_symlink_source else "Unknown"
                    src_size = format_size_human_readable(comp_item.size_source) if comp_item.size_source is not None else "N/A"
                    src_mtime = format_mtime_ns(comp_item.mtime_ns_source)
                    src_sym_target = f" -> '{comp_item.symlink_target_source}'" if comp_item.is_symlink_source and comp_item.symlink_target_source else ""
                    print(f"        Source: Type: {src_type}{src_sym_target}, Size: {src_size}, MTime: {src_mtime}")
                if comp_item.exists_target:
                    tgt_type = "Dir" if comp_item.is_dir_target else "File" if comp_item.is_file_target else "Symlink" if comp_item.is_symlink_target else "Unknown"
                    tgt_size = format_size_human_readable(comp_item.size_target) if comp_item.size_target is not None else "N/A"
                    tgt_mtime = format_mtime_ns(comp_item.mtime_ns_target)
                    tgt_sym_target = f" -> '{comp_item.symlink_target_target}'" if comp_item.is_symlink_target and comp_item.symlink_target_target else ""
                    print(f"        Target: Type: {tgt_type}{tgt_sym_target}, Size: {tgt_size}, MTime: {tgt_mtime}")

            elif category_key in [REPORT_BROKEN_SYMLINK_SOURCE, REPORT_BROKEN_SYMLINK_TARGET]:
                symlink_info = ""
                if category_key == REPORT_BROKEN_SYMLINK_SOURCE:
                    size = format_size_human_readable(comp_item.size_source) if comp_item.size_source is not None else "N/A"
                    mtime = format_mtime_ns(comp_item.mtime_ns_source)
                    target = comp_item.symlink_target_source if comp_item.symlink_target_source else "N/A"
                    symlink_info = f" (size: {size}, mtime: {mtime}, target: '{target}')"
                else: # REPORT_BROKEN_SYMLINK_TARGET
                    size = format_size_human_readable(comp_item.size_target) if comp_item.size_target is not None else "N/A"
                    mtime = format_mtime_ns(comp_item.mtime_ns_target)
                    target = comp_item.symlink_target_target if comp_item.symlink_target_target else "N/A"
                    symlink_info = f" (size: {size}, mtime: {mtime}, target: '{target}')"
                print(f"      {item_path}{symlink_info}")
            elif category_key == REPORT_DIFFERENT_SYMLINK_TARGET:
                print(f"      {item_path}")
                print(f"        Source Target: '{comp_item.symlink_target_source}'")
                print(f"        Target Target: '{comp_item.symlink_target_target}'")
            elif category_key == REPORT_COMPARE_ERROR:
                error_message = comp_item.comparison_error_message or "Unknown Error"
                print(f"      {item_path} (Error: {error_message})")
                # Optionally add scan errors if present
                if comp_item.scan_error_source:
                    print(f"        Source Scan Error: {comp_item.scan_error_source}")
                if comp_item.scan_error_target:
                    print(f"        Target Scan Error: {comp_item.scan_error_target}")
    print() # Add a newline after details


def print_report(
    comparison_results_dict,
    args,
    source_folder_cleaned,
    target_folder_cleaned,
    source_items_scanned, # Keep for consistency, but less direct use needed
    target_items_scanned, # Keep for consistency, but less direct use needed
    elapsed_time_cpu,
    local_date_str,
    SCRIPT_VERSION,
    SCRIPT_DATE,
    all_exclude_patterns,
    num_patterns_from_exclude_file
):
    max_label_len = len("Exclusion patterns loaded from file")

    print("\n" + "="*80)
    print("--------------------------- Folder Comparison Report ---------------------------")
    print("="*80)

    print(f"\n{'Date':<{max_label_len}}: {local_date_str}")
    print(f"{'folder_comparator.py':<{max_label_len}}: version {SCRIPT_VERSION} ({SCRIPT_DATE})")
    print(f"{'source_dir':<{max_label_len}}: {source_folder_cleaned}")
    print(f"{'target_dir':<{max_label_len}}: {target_folder_cleaned}")

    if all_exclude_patterns:
        print(f"{'Exclusion patterns defined':<{max_label_len}}: {len(all_exclude_patterns)} (from CLI: {len(all_exclude_patterns) - num_patterns_from_exclude_file}, from file: {num_patterns_from_exclude_file})")
        # if len(all_exclude_patterns) <= 10: # Only print if few patterns
        #     for pattern in all_exclude_patterns:
        #         print(f"{'':<{max_label_len+2}}- {pattern}")
    else:
        print(f"{'Exclusion patterns defined':<{max_label_len}}: 0")

    if args.time_tolerance is not None:
        print(f"{'Time Tolerance':<{max_label_len}}: {args.time_tolerance:.2f} seconds")
    if args.hash_check:
        print(f"{'Hash Check':<{max_label_len}}: Enabled ({args.hash_check.upper()})")
    if args.ignore_case:
        print(f"{'Case-Insensitive Mode':<{max_label_len}}: Enabled")

    print("\n" + "="*80)
    print("--------------------------------- Report Summary ---------------------------------")
    print("="*80)

    report_categories_fixed_order = [
        (REPORT_BROKEN_SYMLINK_SOURCE, "1. Broken Symbolic Links In Source"),
        (REPORT_BROKEN_SYMLINK_TARGET, "2. Broken Symbolic Links In Target"),
        (REPORT_ONLY_IN_SOURCE_FILE, "3. Files Only In Source"),
        (REPORT_ONLY_IN_SOURCE_DIR, "4. Folders Only In Source"),
        (REPORT_ONLY_IN_TARGET_FILE, "5. Files Only In Target"),
        (REPORT_ONLY_IN_TARGET_DIR, "6. Folders Only In Target"),
        (REPORT_IDENTICAL, "7. Files existing in Source AND Target that are IDENTICAL"),
        (REPORT_IDENTICAL_DIR, "8. Folders existing in Source AND Target that are IDENTICAL"),
        (REPORT_IDENTICAL_SYMLINK, "9. Symbolic Links existing in Source AND Target that are IDENTICAL"),
        (REPORT_DIFFERENT_CONTENT_SOURCE_NEWER, "10. Files existing in Source AND Target that are DIFFERENT (Source Newer)"),
        (REPORT_DIFFERENT_CONTENT_TARGET_NEWER, "11. Files existing in Source AND Target that are DIFFERENT (Target Newer)"),
        (REPORT_DIFFERENT_SYMLINK_TARGET, "12. Symbolic Links existing in Source AND Target that have DIFFERENT Target"),
        (REPORT_EXCLUDED, "13. Excluded Items"),
        (REPORT_DIFFERENT_TYPE, "14. Items existing in Source AND Target that have DIFFERENT Type (File vs. Folder vs. Symlink)"),
        (REPORT_COMPARE_ERROR, "15. Comparison Errors")
    ]

    print() # Add a newline for spacing

    for i, (category_key, category_title_prefix) in enumerate(report_categories_fixed_order):
        # Dynamically adapt category_title for REPORT_IDENTICAL based on hash_check
        display_category_title = category_title_prefix
        if category_key == REPORT_IDENTICAL:
            hash_part = ", AND identical hash." if args.hash_check else ", NO HASH-CHECK."
            display_category_title = (
                f"{category_title_prefix}\n"
                f"{' ':4}(same size AND timestamps within tolerance{hash_part})"
            )
        elif category_key == REPORT_IDENTICAL_DIR:
             display_category_title = f"{category_title_prefix}\n{' ':4}(exist in both and are both directories)"
        elif category_key == REPORT_IDENTICAL_SYMLINK:
             display_category_title = f"{category_title_prefix}\n{' ':4}(exist in both and point to the same target)"
        elif category_key == REPORT_DIFFERENT_SYMLINK_TARGET:
             display_category_title = f"{category_title_prefix}\n{' ':4}(exist in both but point to different targets)"
        elif category_key == REPORT_DIFFERENT_TYPE:
             display_category_title = f"{category_title_prefix}\n{' ':4}(e.g., file in source is a folder in target, or vice versa)"

        print(f"{display_category_title}")

        items = comparison_results_dict.get(category_key, [])

        _print_category_details(category_key, items, source_items_scanned, target_items_scanned,
                                source_folder_cleaned, target_folder_cleaned, args.hash_check,
                                i + 1) # Pass the actual category number (i + 1)


    print(f"\nTotal CPU elapsed wall clock time: {elapsed_time_cpu:.2f} seconds")
    print("\n" + "="*80)
    print("-------------------------------- End of Report ---------------------------------")
    print("="*80)


def main():
    print()

    # --- Python Version Check ---
    # Define the minimum required Python version (e.g., 3.8 for standard fnmatch.translate behavior)
    MIN_PYTHON_VERSION = (3, 8)
    if sys.version_info < MIN_PYTHON_VERSION:
        print(f"Error: This script requires Python {MIN_PYTHON_VERSION[0]}.{MIN_PYTHON_VERSION[1]} or higher.", file=sys.stderr)
        print(f"You are currently using Python {sys.version_info.major}.{sys.version_info.minor}.", file=sys.stderr)
        print("Please use `python3.11` if available, or upgrade your Python version.", file=sys.stderr)
        sys.exit(1)

    start_time_cpu = time.perf_counter() # Use perf_counter for CPU time
    local_date_str, utc_date_str = get_formatted_dates()

    # Initialize the dictionary to store comparison results
    comparison_results_dict = {
        REPORT_BROKEN_SYMLINK_SOURCE: [],
        REPORT_BROKEN_SYMLINK_TARGET: [],
        REPORT_ONLY_IN_SOURCE: [],
        REPORT_ONLY_IN_TARGET: [],
        REPORT_IDENTICAL: [],
        REPORT_DIFFERENT_CONTENT_SOURCE_NEWER: [],
        REPORT_DIFFERENT_CONTENT_TARGET_NEWER: [],
        REPORT_EXCLUDED: [],
        REPORT_DIFFERENT_TYPE: [],
        REPORT_COMPARE_ERROR: []
    }

    args = getArguments()

    # Set global debug flags based on arguments
    DEBUG_EXCLUDE_ENABLED = args.debug_exclude
    DEBUG_TARGET_PATTERNS = args.debug_exclude_filter_patterns

    global all_exclude_patterns
    global num_patterns_from_exclude_file

    # Add patterns from command line
    if args.exclude:
        all_exclude_patterns.extend(args.exclude)

    # Add patterns from exclude file
    if args.exclude_file:
        try:
            if os.path.exists(args.exclude_file):
                patterns_read_from_file_temp = [] # Use a temporary list to count patterns in the file
                with open(args.exclude_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        stripped_line = line.strip()
                        if stripped_line and not stripped_line.startswith('#'):
                            patterns_read_from_file_temp.append(stripped_line)

                num_patterns_from_exclude_file = len(patterns_read_from_file_temp) # Store the number of patterns in the file
                all_exclude_patterns.extend(patterns_read_from_file_temp) # Add these patterns to the global list
            else:
                print(f"Error: Exclusion file '{args.exclude_file}' not found.", file=sys.stderr)
                sys.exit(1)
        except Exception as e:
            print(f"Error reading exclusion file '{args.exclude_file}': {e}", file=sys.stderr)
            sys.exit(1)

    # Clean and validate folder paths
    source_folder_cleaned = os.path.abspath(args.source_folder)
    target_folder_cleaned = os.path.abspath(args.target_folder)

    if not os.path.isdir(source_folder_cleaned):
        print(f"Error: Source folder '{source_folder_cleaned}' not found or is not a directory.", file=sys.stderr)
        sys.exit(1)

    if not os.path.isdir(target_folder_cleaned):
        print(f"Error: Target folder '{target_folder_cleaned}' not found or is not a directory.", file=sys.stderr)
        sys.exit(1)
    
    # Compile patterns once before comparison starts
    # This call sets patterns_compiled_flag = True and populates global lists
    compiled_file_patterns_global, compiled_dir_patterns_global = compile_exclusion_patterns(all_exclude_patterns)
    patterns_compiled_flag = True # Set flag after compilation

    # Perform initial scans to populate broken symlinks and excluded items
    # and get lists of non-excluded items for comparison
    source_items_scanned = _scan_folder(source_folder_cleaned, comparison_results_dict, True, args.ignore_case)
    target_items_scanned = _scan_folder(target_folder_cleaned, comparison_results_dict, False, args.ignore_case)

    # Perform comparison of non-excluded items
    comparison_results_dict = compare_scanned_items(source_items_scanned, target_items_scanned, comparison_results_dict, args.time_tolerance, args.hash_check)

    end_time_cpu = time.perf_counter() #
    elapsed_time_cpu = end_time_cpu - start_time_cpu #

    # --- Print Comparison Report ---
    print_report(
        comparison_results_dict,
        args,
        source_folder_cleaned,
        target_folder_cleaned,
        source_items_scanned,
        target_items_scanned,
        elapsed_time_cpu,
        local_date_str,
        SCRIPT_VERSION,
        SCRIPT_DATE,
        all_exclude_patterns,
        num_patterns_from_exclude_file,
        REPORT_BROKEN_SYMLINK_SOURCE,
        REPORT_BROKEN_SYMLINK_TARGET,
        REPORT_ONLY_IN_SOURCE,
        REPORT_ONLY_IN_TARGET,
        REPORT_IDENTICAL,
        REPORT_DIFFERENT_CONTENT_SOURCE_NEWER,
        REPORT_DIFFERENT_CONTENT_TARGET_NEWER,
        REPORT_EXCLUDED,
        REPORT_DIFFERENT_TYPE,
        REPORT_COMPARE_ERROR
    )

if __name__ == "__main__":
    main()
